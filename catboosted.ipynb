{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d443347",
   "metadata": {},
   "source": [
    "## 0) Building TF using tides "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cfb8bd",
   "metadata": {},
   "source": [
    "## a) Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a65a9b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading klines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 29.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Indicators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:26<00:00,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data.klines_management import klines_manager\n",
    "\n",
    "resample = True\n",
    "update_db = False\n",
    "timeframes = [f\"{i}h\" for i in range(1,13)]\n",
    "config = {\"general\":{\"klines_db_location\":\"/Users/Shaik Reza Shafiq/Desktop/Tide/database/TV_klines.db\",\n",
    "                     \"output\": \"telegram/\"},\n",
    "          \"strategy\": {\"timeframes\": timeframes,\n",
    "                      \"indicators\": {'tide': {'window': [5,20,67], \"sensitivity\": [10], \"thresholds\": [5]},\n",
    "                                     'atr': {'length': [14]},\n",
    "                                     'mfi': {'length': [14], 'close': ['close']},\n",
    "                                     'ema': {'length': [39], 'close': ['close']}\n",
    "                                     },\n",
    "                      },\n",
    "          \n",
    "          }\n",
    "\n",
    "\n",
    "\n",
    "Klines_Manager = klines_manager(timeframes=config[\"strategy\"][\"timeframes\"],\n",
    "                                indicators=config[\"strategy\"][\"indicators\"],\n",
    "                                klines_db_location=config[\"general\"][\"klines_db_location\"],\n",
    "                                resample = resample,\n",
    "                                update_db = update_db)\n",
    "\n",
    "instruments =[\"CME_BTC1!\",\n",
    "              \"SGX_CN1!\",\n",
    "              \"SGX_TWN1!\",\n",
    "              \"SGX_SGP1!\",\n",
    "              \"HKEX_HSI1!\",\n",
    "              \"HKEX_TCH1!\",\n",
    "              \"HKEX_ALB1!\",\n",
    "              \"COMEX_MINI_MGC1!\",\n",
    "              \"NASDAQ_TSLA\",\n",
    "              \"NASDAQ_NFLX\",\n",
    "              \"NYSE_SE\",\n",
    "              \"CME_MINI_ES1!\",\n",
    "              \"CME_MINI_NQ1!\"\n",
    "              ]\n",
    "\n",
    "Klines_Manager.load_equities_from_db(instruments)\n",
    "\n",
    "Klines_Manager.calc_indicators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "936044cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4h_open</th>\n",
       "      <th>4h_high</th>\n",
       "      <th>4h_low</th>\n",
       "      <th>4h_close</th>\n",
       "      <th>4h_volume</th>\n",
       "      <th>4h_closeTime</th>\n",
       "      <th>4h_tide</th>\n",
       "      <th>4h_tide_id</th>\n",
       "      <th>4h_ebb</th>\n",
       "      <th>4h_flow</th>\n",
       "      <th>...</th>\n",
       "      <th>12h_close</th>\n",
       "      <th>12h_volume</th>\n",
       "      <th>12h_closeTime</th>\n",
       "      <th>12h_tide</th>\n",
       "      <th>12h_tide_id</th>\n",
       "      <th>12h_ebb</th>\n",
       "      <th>12h_flow</th>\n",
       "      <th>12h_ATRr_14</th>\n",
       "      <th>12h_MFI_14</th>\n",
       "      <th>12h_EMA_39</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-06 17:15:00</th>\n",
       "      <td>1302.50</td>\n",
       "      <td>1305.75</td>\n",
       "      <td>1298.25</td>\n",
       "      <td>1301.75</td>\n",
       "      <td>3730.0</td>\n",
       "      <td>1.609953e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1298.250000</td>\n",
       "      <td>1298.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>1301.75</td>\n",
       "      <td>75100.0</td>\n",
       "      <td>1.609953e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1289.250000</td>\n",
       "      <td>1289.250000</td>\n",
       "      <td>21.635533</td>\n",
       "      <td>38.531171</td>\n",
       "      <td>1303.713248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-06 18:15:00</th>\n",
       "      <td>1304.25</td>\n",
       "      <td>1305.75</td>\n",
       "      <td>1298.25</td>\n",
       "      <td>1302.00</td>\n",
       "      <td>4205.0</td>\n",
       "      <td>1.609957e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1298.250000</td>\n",
       "      <td>1298.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>1302.00</td>\n",
       "      <td>76017.0</td>\n",
       "      <td>1.609957e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1289.250000</td>\n",
       "      <td>1289.250000</td>\n",
       "      <td>22.441217</td>\n",
       "      <td>44.454972</td>\n",
       "      <td>1303.627586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-06 19:15:00</th>\n",
       "      <td>1302.50</td>\n",
       "      <td>1303.50</td>\n",
       "      <td>1298.25</td>\n",
       "      <td>1303.25</td>\n",
       "      <td>4112.0</td>\n",
       "      <td>1.609960e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1298.250000</td>\n",
       "      <td>1298.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>1303.25</td>\n",
       "      <td>76083.0</td>\n",
       "      <td>1.609960e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1289.250000</td>\n",
       "      <td>1289.250000</td>\n",
       "      <td>23.187705</td>\n",
       "      <td>49.359486</td>\n",
       "      <td>1303.608706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-06 20:15:00</th>\n",
       "      <td>1300.50</td>\n",
       "      <td>1303.50</td>\n",
       "      <td>1300.50</td>\n",
       "      <td>1302.00</td>\n",
       "      <td>2984.0</td>\n",
       "      <td>1.609964e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1299.471429</td>\n",
       "      <td>1300.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1302.00</td>\n",
       "      <td>66002.0</td>\n",
       "      <td>1.609964e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1289.250000</td>\n",
       "      <td>1289.250000</td>\n",
       "      <td>23.879457</td>\n",
       "      <td>44.444254</td>\n",
       "      <td>1303.528271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-06 21:15:00</th>\n",
       "      <td>1302.00</td>\n",
       "      <td>1303.75</td>\n",
       "      <td>1300.50</td>\n",
       "      <td>1301.25</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>1.609968e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1299.471429</td>\n",
       "      <td>1300.542857</td>\n",
       "      <td>...</td>\n",
       "      <td>1301.25</td>\n",
       "      <td>49698.0</td>\n",
       "      <td>1.609968e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1289.250000</td>\n",
       "      <td>1289.250000</td>\n",
       "      <td>24.177271</td>\n",
       "      <td>41.009526</td>\n",
       "      <td>1303.414357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-28 02:15:00</th>\n",
       "      <td>1354.00</td>\n",
       "      <td>1356.25</td>\n",
       "      <td>1350.50</td>\n",
       "      <td>1353.50</td>\n",
       "      <td>2414.0</td>\n",
       "      <td>1.656382e+09</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>1357.792857</td>\n",
       "      <td>1355.757143</td>\n",
       "      <td>...</td>\n",
       "      <td>1353.50</td>\n",
       "      <td>15640.0</td>\n",
       "      <td>1.656382e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>1347.085714</td>\n",
       "      <td>1353.407143</td>\n",
       "      <td>17.098394</td>\n",
       "      <td>61.420089</td>\n",
       "      <td>1350.868069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-28 03:15:00</th>\n",
       "      <td>1352.25</td>\n",
       "      <td>1354.00</td>\n",
       "      <td>1350.50</td>\n",
       "      <td>1352.75</td>\n",
       "      <td>1793.0</td>\n",
       "      <td>1.656386e+09</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>1356.035714</td>\n",
       "      <td>1354.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1352.75</td>\n",
       "      <td>14641.0</td>\n",
       "      <td>1.656386e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>1347.085714</td>\n",
       "      <td>1353.364286</td>\n",
       "      <td>16.805652</td>\n",
       "      <td>65.654989</td>\n",
       "      <td>1350.962165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-28 04:15:00</th>\n",
       "      <td>1353.00</td>\n",
       "      <td>1354.00</td>\n",
       "      <td>1349.75</td>\n",
       "      <td>1352.75</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>1.656390e+09</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>1355.950000</td>\n",
       "      <td>1354.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1352.75</td>\n",
       "      <td>14137.0</td>\n",
       "      <td>1.656390e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>1347.085714</td>\n",
       "      <td>1353.128571</td>\n",
       "      <td>16.337391</td>\n",
       "      <td>59.841064</td>\n",
       "      <td>1351.051557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-28 08:15:00</th>\n",
       "      <td>1353.00</td>\n",
       "      <td>1354.75</td>\n",
       "      <td>1349.75</td>\n",
       "      <td>1357.00</td>\n",
       "      <td>1716.0</td>\n",
       "      <td>1.656404e+09</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>1355.950000</td>\n",
       "      <td>1354.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1357.00</td>\n",
       "      <td>13292.0</td>\n",
       "      <td>1.656404e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>1347.085714</td>\n",
       "      <td>1352.871429</td>\n",
       "      <td>15.884720</td>\n",
       "      <td>55.933524</td>\n",
       "      <td>1351.348979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-28 09:15:00</th>\n",
       "      <td>1353.00</td>\n",
       "      <td>1359.75</td>\n",
       "      <td>1349.75</td>\n",
       "      <td>1351.00</td>\n",
       "      <td>7167.0</td>\n",
       "      <td>1.656408e+09</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>1355.950000</td>\n",
       "      <td>1354.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1351.00</td>\n",
       "      <td>18554.0</td>\n",
       "      <td>1.656408e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>1347.085714</td>\n",
       "      <td>1352.785714</td>\n",
       "      <td>15.553669</td>\n",
       "      <td>61.291211</td>\n",
       "      <td>1351.331530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8056 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     4h_open  4h_high   4h_low  4h_close  4h_volume  \\\n",
       "date_time                                                             \n",
       "2021-01-06 17:15:00  1302.50  1305.75  1298.25   1301.75     3730.0   \n",
       "2021-01-06 18:15:00  1304.25  1305.75  1298.25   1302.00     4205.0   \n",
       "2021-01-06 19:15:00  1302.50  1303.50  1298.25   1303.25     4112.0   \n",
       "2021-01-06 20:15:00  1300.50  1303.50  1300.50   1302.00     2984.0   \n",
       "2021-01-06 21:15:00  1302.00  1303.75  1300.50   1301.25     1988.0   \n",
       "...                      ...      ...      ...       ...        ...   \n",
       "2022-06-28 02:15:00  1354.00  1356.25  1350.50   1353.50     2414.0   \n",
       "2022-06-28 03:15:00  1352.25  1354.00  1350.50   1352.75     1793.0   \n",
       "2022-06-28 04:15:00  1353.00  1354.00  1349.75   1352.75     1983.0   \n",
       "2022-06-28 08:15:00  1353.00  1354.75  1349.75   1357.00     1716.0   \n",
       "2022-06-28 09:15:00  1353.00  1359.75  1349.75   1351.00     7167.0   \n",
       "\n",
       "                     4h_closeTime  4h_tide  4h_tide_id       4h_ebb  \\\n",
       "date_time                                                             \n",
       "2021-01-06 17:15:00  1.609953e+09      1.0         1.0  1298.250000   \n",
       "2021-01-06 18:15:00  1.609957e+09      1.0         1.0  1298.250000   \n",
       "2021-01-06 19:15:00  1.609960e+09      1.0         1.0  1298.250000   \n",
       "2021-01-06 20:15:00  1.609964e+09      1.0         1.0  1299.471429   \n",
       "2021-01-06 21:15:00  1.609968e+09      1.0         1.0  1299.471429   \n",
       "...                           ...      ...         ...          ...   \n",
       "2022-06-28 02:15:00  1.656382e+09     -1.0       444.0  1357.792857   \n",
       "2022-06-28 03:15:00  1.656386e+09     -1.0       444.0  1356.035714   \n",
       "2022-06-28 04:15:00  1.656390e+09     -1.0       444.0  1355.950000   \n",
       "2022-06-28 08:15:00  1.656404e+09     -1.0       444.0  1355.950000   \n",
       "2022-06-28 09:15:00  1.656408e+09     -1.0       444.0  1355.950000   \n",
       "\n",
       "                         4h_flow  ...  12h_close  12h_volume  12h_closeTime  \\\n",
       "date_time                         ...                                         \n",
       "2021-01-06 17:15:00  1298.250000  ...    1301.75     75100.0   1.609953e+09   \n",
       "2021-01-06 18:15:00  1298.250000  ...    1302.00     76017.0   1.609957e+09   \n",
       "2021-01-06 19:15:00  1298.250000  ...    1303.25     76083.0   1.609960e+09   \n",
       "2021-01-06 20:15:00  1300.500000  ...    1302.00     66002.0   1.609964e+09   \n",
       "2021-01-06 21:15:00  1300.542857  ...    1301.25     49698.0   1.609968e+09   \n",
       "...                          ...  ...        ...         ...            ...   \n",
       "2022-06-28 02:15:00  1355.757143  ...    1353.50     15640.0   1.656382e+09   \n",
       "2022-06-28 03:15:00  1354.000000  ...    1352.75     14641.0   1.656386e+09   \n",
       "2022-06-28 04:15:00  1354.000000  ...    1352.75     14137.0   1.656390e+09   \n",
       "2022-06-28 08:15:00  1354.000000  ...    1357.00     13292.0   1.656404e+09   \n",
       "2022-06-28 09:15:00  1354.000000  ...    1351.00     18554.0   1.656408e+09   \n",
       "\n",
       "                     12h_tide  12h_tide_id      12h_ebb     12h_flow  \\\n",
       "date_time                                                              \n",
       "2021-01-06 17:15:00       1.0          1.0  1289.250000  1289.250000   \n",
       "2021-01-06 18:15:00       1.0          1.0  1289.250000  1289.250000   \n",
       "2021-01-06 19:15:00       1.0          1.0  1289.250000  1289.250000   \n",
       "2021-01-06 20:15:00       1.0          1.0  1289.250000  1289.250000   \n",
       "2021-01-06 21:15:00       1.0          1.0  1289.250000  1289.250000   \n",
       "...                       ...          ...          ...          ...   \n",
       "2022-06-28 02:15:00       1.0        177.0  1347.085714  1353.407143   \n",
       "2022-06-28 03:15:00       1.0        177.0  1347.085714  1353.364286   \n",
       "2022-06-28 04:15:00       1.0        177.0  1347.085714  1353.128571   \n",
       "2022-06-28 08:15:00       1.0        177.0  1347.085714  1352.871429   \n",
       "2022-06-28 09:15:00       1.0        177.0  1347.085714  1352.785714   \n",
       "\n",
       "                     12h_ATRr_14  12h_MFI_14   12h_EMA_39  \n",
       "date_time                                                  \n",
       "2021-01-06 17:15:00    21.635533   38.531171  1303.713248  \n",
       "2021-01-06 18:15:00    22.441217   44.454972  1303.627586  \n",
       "2021-01-06 19:15:00    23.187705   49.359486  1303.608706  \n",
       "2021-01-06 20:15:00    23.879457   44.444254  1303.528271  \n",
       "2021-01-06 21:15:00    24.177271   41.009526  1303.414357  \n",
       "...                          ...         ...          ...  \n",
       "2022-06-28 02:15:00    17.098394   61.420089  1350.868069  \n",
       "2022-06-28 03:15:00    16.805652   65.654989  1350.962165  \n",
       "2022-06-28 04:15:00    16.337391   59.841064  1351.051557  \n",
       "2022-06-28 08:15:00    15.884720   55.933524  1351.348979  \n",
       "2022-06-28 09:15:00    15.553669   61.291211  1351.331530  \n",
       "\n",
       "[8056 rows x 117 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from strategy.indicators import calc_tides, calc_continuous_resample\n",
    "# klines = calc_continuous_resample(df,4 )\n",
    "df0 = Klines_Manager.klines_indicators_dict[\"SGX_TWN1!\"].copy()\n",
    "df0.drop(columns=list(df0.filter(regex=\"(^1h)|(^2h)|(^3h)\").columns), inplace=True)\n",
    "df0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "695e86fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test=df0.copy()\n",
    "tf = \"23h\"\n",
    "g = test.groupby(f\"{tf}_tide_id\")\n",
    "g.apply(lambda x: x[f\"{tf}_close\"][-1]-x[f\"{tf}_close\"][0]).hist(bins=100)\n",
    "# g.apply(lambda x: x.count()[0]).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50bab54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe768d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa488e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938d390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bf9e923",
   "metadata": {},
   "source": [
    "# 1) Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63adf4",
   "metadata": {},
   "source": [
    "## a) Build features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c26cc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(df):\n",
    "    tfs = [i.split(\"_\")[0] for i in list(df.filter(regex=\"high\"))]\n",
    "    \"\"\"\n",
    "    x-y, x-y, x-y, \n",
    "    for y in [high, low, close]\n",
    "    for x in [open, ebb, flow] for all timeframes\n",
    "    \"\"\"\n",
    "    tides = list(df.filter(regex=\"tide\").columns)\n",
    "    df1 = df[[f\"{tf}_close\" for tf in tfs] +tides] .copy()\n",
    "    for tide in tides:\n",
    "        df1[tide] = df1[tide].astype(\"int\")\n",
    "    for tf in tfs:\n",
    "        for x in [\"open\", \"ebb\",\"flow\"]:\n",
    "            for y in [\"high\", \"low\", \"close\"]:\n",
    "                df1[f\"{tf}_{x}_{y}\"] = df[f\"{tf}_{x}\"] - df[f\"{tf}_{y}\"]\n",
    "                \n",
    "    return df1\n",
    "df1 = build_features(df0)    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2af4f",
   "metadata": {},
   "source": [
    "## b) Build labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2fab2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class tripleBarrier:\n",
    "    def __init__(self,df: pd.DataFrame,\n",
    "                 col_name:str,\n",
    "                 vol_lookback: int = 100,\n",
    "                 vol_delta: pd.Timedelta = pd.Timedelta(hours=1),\n",
    "                 horizon_delta: pd.Timedelta = pd.Timedelta(minutes=15),\n",
    "                 barrier_multiplier: list = [1,1]\n",
    "                ):\n",
    "        self.df = df\n",
    "        self.col_name = col_name\n",
    "        self.vol_lookback = vol_lookback\n",
    "        self.vol_delta = vol_delta\n",
    "        self.horizon_delta = horizon_delta\n",
    "        self.barrier_multiplier = barrier_multiplier\n",
    "        \n",
    "    \"\"\"\n",
    "    main method: assignTripleBarrier()\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    def get_vol(self,data_col, vol_lookback,vol_delta):\n",
    "        prices = data_col.copy()\n",
    "        df0 = prices.index.searchsorted(prices.index - vol_delta)\n",
    "        df0 = df0[df0 > 0]  # 1.2 align timestamps of p[t-1] to timestamps of p[t]\n",
    "        df0 = pd.Series(prices.index[df0-1],   \n",
    "               index=prices.index[prices.shape[0]-df0.shape[0] : ])  # 1.3 get values by timestamps, then compute returns\n",
    "        df0 = prices.loc[df0.index] / prices.loc[df0.values].values - 1 \n",
    "        # 2. estimate rolling standard deviation\n",
    "        df0 = df0.ewm(span=vol_lookback).std()\n",
    "        return df0\n",
    "    \n",
    "    def get_horizons(self,prices, horizon_delta=pd.Timedelta(minutes=15)):\n",
    "        t1 = prices.index.searchsorted(prices.index + horizon_delta)\n",
    "        t1 = t1[t1 < prices.shape[0]]\n",
    "        t1 = prices.index[t1]\n",
    "        t1 = pd.Series(t1, index=prices.index[:t1.shape[0]])\n",
    "        t1.name = \"t1\"\n",
    "        return t1\n",
    "    def get_touches(self,prices, events, barrier_multiplier: list):\n",
    "        '''\n",
    "        events: pd dataframe with columns\n",
    "        t1: timestamp of the next horizon\n",
    "        threshold: unit height of top and bottom barriers\n",
    "        side: the side of each bet\n",
    "        barrier_multiplier: multipliers of the threshold to set the height of \n",
    "               top/bottom barriers\n",
    "        '''\n",
    "        out = events[['t1']].copy(deep=True)\n",
    "        if barrier_multiplier[0] > 0: \n",
    "            thresh_uppr = barrier_multiplier[0] * events['threshold']\n",
    "        else:\n",
    "            thresh_uppr = pd.Series(index=events.index) # no uppr thresh\n",
    "        if barrier_multiplier[1] > 0:\n",
    "            thresh_lwr = -barrier_multiplier[1] * events['threshold']\n",
    "        else:\n",
    "            thresh_lwr = pd.Series(index=events.index)  # no lwr thresh\n",
    "        for loc, t1 in tqdm(events['t1'].iteritems()):\n",
    "            df0=prices[loc:t1]                              # path prices\n",
    "            df0=(df0 / prices[loc] - 1) * events.side[loc]  # path returns\n",
    "            out.loc[loc, 'touch_top'] = df0[df0 < thresh_lwr[loc]].index.min()  # earliest touch_bot\n",
    "            out.loc[loc, 'touch_bot'] = df0[df0 > thresh_uppr[loc]].index.min() # earliest touch_top\n",
    "        return out\n",
    "    \n",
    "    def get_labels(self,touches):\n",
    "        out = touches.copy(deep=True)\n",
    "        # pandas df.min() ignores NaN values\n",
    "        first_touch = touches[['touch_bot', 'touch_top']].min(axis=1)\n",
    "        for loc, t in tqdm(first_touch.iteritems()):\n",
    "            if pd.isnull(t):\n",
    "                out.loc[loc, 'label'] = 0\n",
    "            elif t == touches.loc[loc, 'touch_bot']: \n",
    "                out.loc[loc, 'label'] = -1\n",
    "            else:\n",
    "                out.loc[loc, 'label'] = 1\n",
    "        return out\n",
    "    \n",
    "    def calcTripleBarrier(self):\n",
    "        data = self.df.copy()\n",
    "        data = data.assign(threshold=self.get_vol(data[self.col_name],\n",
    "                                                  vol_lookback = self.vol_lookback,\n",
    "                                                  vol_delta = self.vol_delta).dropna())\n",
    "        t1 = self.get_horizons(data, horizon_delta = self.horizon_delta)\n",
    "        data = pd.merge(data, t1, left_index=True, right_index=True, how=\"left\").dropna()\n",
    "        events_raw = data[['t1', 'threshold']] \n",
    "        events = events_raw.assign(side=pd.Series(-1., events_raw.index)) # long only\n",
    "        touches = self.get_touches(data[self.col_name], events, self.barrier_multiplier)\n",
    "        touches = self.get_labels(touches)\n",
    "        data = data.assign(label=touches.label)\n",
    "        \n",
    "        # for debugging\n",
    "        self.events_raw = events_raw\n",
    "        self.events = events\n",
    "        self.touches =touches\n",
    "        \n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bcd2f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8048it [00:07, 1007.68it/s]\n",
      "8048it [00:00, 10024.27it/s]\n"
     ]
    }
   ],
   "source": [
    "tB = tripleBarrier(df1,\n",
    "                   col_name=\"4h_close\",\n",
    "                   vol_lookback = 10,\n",
    "                   vol_delta = pd.Timedelta(hours=4),\n",
    "                   horizon_delta = pd.Timedelta(hours=4),\n",
    "                   barrier_multiplier = [1,1])\n",
    "df2 = tB.calcTripleBarrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4922f",
   "metadata": {},
   "source": [
    "## c) Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bcf06dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "n = int(len(df0)*0.5)\n",
    "iterations = 100\n",
    "df = df2.drop(columns=[\"threshold\", \"t1\"]).copy()\n",
    "df_train= df.iloc[:n,:].copy()\n",
    "df_test = df.iloc[n:,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc164a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18a36e095cb4633a899771958413da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x2dd39e42910>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cat_features = list(df_train.filter(regex=\"tide\"))\n",
    "features = list(df_train.columns)[:-1]\n",
    "labels = [\"label\"]\n",
    "\n",
    "model = CatBoostClassifier(task_type=\"GPU\",iterations=iterations)\n",
    "model.fit(X=df_train[features], y=df_train[labels],cat_features = cat_features, verbose=False,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ae011e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IST Model Accuracy:  0.9314796425024826\n",
      "OST Model Accuracy:  0.49577114427860697\n",
      "                      0\n",
      "12h_flow_high  4.262589\n",
      "12h_ebb_high   3.180256\n",
      "12h_tide_id    2.906395\n",
      "5h_tide_id     2.855785\n",
      "9h_flow_high   2.734735\n",
      "...                 ...\n",
      "4h_ebb_close   0.146393\n",
      "6h_ebb_low     0.135981\n",
      "5h_flow_close  0.135573\n",
      "7h_close       0.132632\n",
      "9h_tide_id     0.023243\n",
      "\n",
      "[100 rows x 1 columns]\n",
      "               0\n",
      "12h_close    0.0\n",
      "12h_tide     0.0\n",
      "11h_tide     0.0\n",
      "11h_tide_id  0.0\n",
      "9h_tide      0.0\n",
      "8h_tide      0.0\n",
      "7h_tide      0.0\n",
      "10h_tide     0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calc_accuracy(model, features, labels, df_train,df_test, feature_importances=False):\n",
    "    train_y_pred = model.predict(df_train[features])\n",
    "    print(\"IST Model Accuracy: \", accuracy_score(df_train[labels], train_y_pred))\n",
    "\n",
    "    test_y_pred = model.predict(df_test[features])\n",
    "    print(\"OST Model Accuracy: \", accuracy_score(df_test[labels], test_y_pred))\n",
    "    \n",
    "    if feature_importances:\n",
    "        df_fi = pd.DataFrame(model.feature_importances_, index=df_train[features].columns, ).sort_values(by=0,ascending=False)\n",
    "        fi_top = df_fi[df_fi>0.0].dropna()\n",
    "        print(fi_top)\n",
    "        fi_bot = df_fi[df_fi==0.0].dropna()\n",
    "        print(fi_bot)\n",
    "    \n",
    "    return train_y_pred\n",
    "\n",
    "train_y_pred = calc_accuracy(model, features, labels, df_train,df_test,feature_importances=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b3e3a",
   "metadata": {},
   "source": [
    "# 2) Ensemble Catboost \n",
    "Here we predict for all timeframes individually, and use those labels as inputs for final classifier            \n",
    "Questions:\n",
    "- all use same label?\n",
    "        \n",
    "Architecture:\n",
    " - t0-> t   train each tf ta to get tf feature, model_tf\n",
    " - t0-> t   train tfs features + tf ta to get model_T\n",
    " - t -> t+x train each tf ta to get tf feature\n",
    " - t -> t+x model.pred(tf features, tf ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a7bebfe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['4h_close', '5h_close', '6h_close', '7h_close', '8h_close', '9h_close',\n",
       "       '10h_close', '11h_close', '12h_close', '4h_tide',\n",
       "       ...\n",
       "       '12h_open_close', '12h_ebb_high', '12h_ebb_low', '12h_ebb_close',\n",
       "       '12h_flow_high', '12h_flow_low', '12h_flow_close', 'threshold', 't1',\n",
       "       'label'],\n",
       "      dtype='object', length=102)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "931d7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "def run_classifier(df,timeframe=\"all\", sample=0.5):\n",
    "        \n",
    "    df0=df.copy()\n",
    "    \n",
    "    n = int(len(df0)*sample)\n",
    "    iterations = 100\n",
    "    \n",
    "    if timeframe == \"all\":\n",
    "        timeframes =np.unique([tf.split(\"_\")[0] for tf in df0.filter(regex=\"h_open\").columns])\n",
    "        print(timeframes)\n",
    "    else:\n",
    "        timeframes = [timeframe]\n",
    "    df0 = df0.drop(columns=[\"threshold\", \"t1\"])\n",
    "    df_train_tide2 = pd.DataFrame()\n",
    "    df_train_tide2.index = df0.index[:n]\n",
    "    \n",
    "    df_test_tide2 = pd.DataFrame()\n",
    "    df_test_tide2.index = df0.index[n:]\n",
    "    \n",
    "    models = {}\n",
    "    for tf in timeframes:\n",
    "        print(f\"Getting tide2 {tf}\")\n",
    "        df_t = df0.copy().filter(regex=f\"{tf}|(label)\")\n",
    "        df_t = df_t.drop(columns=[f\"{tf}_close\"])\n",
    "\n",
    "        df_train_t = df_t.iloc[:n,:].copy()\n",
    "        df_test_t = df_t.iloc[n:,:].copy()\n",
    "\n",
    "        cat_features_t = list(df_train_t.filter(regex=\"tide\"))\n",
    "        features_t = list(df_train_t.columns)[:-1]\n",
    "        labels = [\"label\"]\n",
    "\n",
    "        model_t = CatBoostClassifier(task_type=\"GPU\",iterations=iterations)\n",
    "        model_t.fit(X=df_train_t[features_t], y=df_train_t[labels],cat_features = cat_features_t, verbose=False,plot=False)\n",
    "        models[tf] = model_t\n",
    "        \n",
    "        train_tide2 = model_t.predict(df_train_t[features_t])\n",
    "        test_tide2 = model_t.predict(df_test_t[features_t])\n",
    "        \n",
    "        df_train_tide2[f\"{tf}_tide2\"] = train_tide2\n",
    "        df_test_tide2[f\"{tf}_tide2\"] = test_tide2\n",
    "        \n",
    "    # Final fit using train pred\n",
    "    print(f\"Preparing for Final FIT\")\n",
    "    df_T = df0.drop(columns=list(df0.filter(regex=\"h_close\").columns))\n",
    "    df_tide2 = pd.concat([df_train_tide2,df_test_tide2])\n",
    "    df_T = pd.merge(df_T, df_tide2, right_index=True, left_index=True, how=\"left\")\n",
    "    \n",
    "    for tide2 in df_train_tide2.columns:\n",
    "        try:\n",
    "            df_T[tide2] = df_T[tide2].astype(\"int\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return {\"df_T\":df_T,\"df_tide2\":df_tide2 }\n",
    "        \n",
    "    df_train_T = df_T.iloc[:n,:].copy()\n",
    "    df_test_T = df_T.iloc[n:,:].copy()\n",
    "\n",
    "    cat_features_T = list(df_train_T.filter(regex=\"(tide)|(h_pred)\"))\n",
    "    features_T = list(df_train_T.drop(columns=[\"label\"]).columns)\n",
    "    labels = [\"label\"]\n",
    "    \n",
    "    print(\"FITTING FINAL MODEL\")\n",
    "\n",
    "    model_T = CatBoostClassifier(task_type=\"GPU\",iterations=iterations)\n",
    "    model_T.fit(X=df_train_T[features_T], y=df_train_T[labels],cat_features = cat_features_T, verbose=False,plot=False) ### \n",
    "    \n",
    "    train_y_pred = calc_accuracy(model_T, features_T, labels, df_train_T,df_test_T)\n",
    "    return {\"model\":model_T, \"df_T\":df_T,\"df_tide2\":df_tide2 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4f4efbf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10h' '11h' '12h' '4h' '5h' '6h' '7h' '8h' '9h']\n",
      "Getting tide2 10h\n",
      "Getting tide2 11h\n",
      "Getting tide2 12h\n",
      "Getting tide2 4h\n",
      "Getting tide2 5h\n",
      "Getting tide2 6h\n",
      "Getting tide2 7h\n",
      "Getting tide2 8h\n",
      "Getting tide2 9h\n",
      "Preparing for Final FIT\n",
      "FITTING FINAL MODEL\n",
      "IST Model Accuracy:  0.9937749003984063\n",
      "OST Model Accuracy:  0.48892208115509084\n"
     ]
    }
   ],
   "source": [
    "res = run_classifier(df2,timeframe=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8aa7ac8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4h_tide</th>\n",
       "      <th>5h_tide</th>\n",
       "      <th>6h_tide</th>\n",
       "      <th>7h_tide</th>\n",
       "      <th>8h_tide</th>\n",
       "      <th>9h_tide</th>\n",
       "      <th>10h_tide</th>\n",
       "      <th>11h_tide</th>\n",
       "      <th>12h_tide</th>\n",
       "      <th>4h_open_high</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>10h_tide2</th>\n",
       "      <th>11h_tide2</th>\n",
       "      <th>12h_tide2</th>\n",
       "      <th>4h_tide2</th>\n",
       "      <th>5h_tide2</th>\n",
       "      <th>6h_tide2</th>\n",
       "      <th>7h_tide2</th>\n",
       "      <th>8h_tide2</th>\n",
       "      <th>9h_tide2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-06 23:15:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-07 00:15:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-07 01:15:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.50</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-07 02:15:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-07 03:15:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-27 09:15:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.50</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-27 10:15:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-10.50</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-27 11:15:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.25</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-27 12:15:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.25</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-27 13:15:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8033 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     4h_tide  5h_tide  6h_tide  7h_tide  8h_tide  9h_tide  \\\n",
       "date_time                                                                   \n",
       "2021-01-06 23:15:00        1        1        1        1        1        1   \n",
       "2021-01-07 00:15:00        1        1        1        1        1        1   \n",
       "2021-01-07 01:15:00        1        1        1        1        1        1   \n",
       "2021-01-07 02:15:00        1        1        1        1        1        1   \n",
       "2021-01-07 03:15:00        1        1        1        1        1        1   \n",
       "...                      ...      ...      ...      ...      ...      ...   \n",
       "2022-06-27 09:15:00        1        1        1        1        1        1   \n",
       "2022-06-27 10:15:00        1        1        1        1        1        1   \n",
       "2022-06-27 11:15:00        1        1        1        1        1        1   \n",
       "2022-06-27 12:15:00        1        1        1        1        1        1   \n",
       "2022-06-27 13:15:00        1        1        1        1        1        1   \n",
       "\n",
       "                     10h_tide  11h_tide  12h_tide  4h_open_high  ...  label  \\\n",
       "date_time                                                        ...          \n",
       "2021-01-06 23:15:00         1         1         1         -4.00  ...    1.0   \n",
       "2021-01-07 00:15:00         1         1         1         -8.50  ...    1.0   \n",
       "2021-01-07 01:15:00         1         1         1         -9.50  ...    1.0   \n",
       "2021-01-07 02:15:00         1         1         1         -6.75  ...    0.0   \n",
       "2021-01-07 03:15:00         1         1         1         -6.50  ...    0.0   \n",
       "...                       ...       ...       ...           ...  ...    ...   \n",
       "2022-06-27 09:15:00         1         1         1         -7.50  ...   -1.0   \n",
       "2022-06-27 10:15:00         1         1         1        -10.50  ...   -1.0   \n",
       "2022-06-27 11:15:00         1         1         1        -15.25  ...   -1.0   \n",
       "2022-06-27 12:15:00         1         1         1         -7.25  ...   -1.0   \n",
       "2022-06-27 13:15:00         1         1         1         -6.00  ...    0.0   \n",
       "\n",
       "                     10h_tide2  11h_tide2  12h_tide2  4h_tide2  5h_tide2  \\\n",
       "date_time                                                                  \n",
       "2021-01-06 23:15:00          1          1          1         0         1   \n",
       "2021-01-07 00:15:00          1          1          0         1         1   \n",
       "2021-01-07 01:15:00          1          1          0         0         1   \n",
       "2021-01-07 02:15:00          0          0          0         0         0   \n",
       "2021-01-07 03:15:00          0          0          0         0         0   \n",
       "...                        ...        ...        ...       ...       ...   \n",
       "2022-06-27 09:15:00          0          0          0         0         0   \n",
       "2022-06-27 10:15:00          0          0          0         0         0   \n",
       "2022-06-27 11:15:00          0          0          0         0         0   \n",
       "2022-06-27 12:15:00          0          0          0         0         0   \n",
       "2022-06-27 13:15:00          0          0          1         0         0   \n",
       "\n",
       "                     6h_tide2  7h_tide2  8h_tide2  9h_tide2  \n",
       "date_time                                                    \n",
       "2021-01-06 23:15:00         1         1         0         0  \n",
       "2021-01-07 00:15:00         1         0         0         0  \n",
       "2021-01-07 01:15:00         0         0         0         1  \n",
       "2021-01-07 02:15:00         0         0         0         0  \n",
       "2021-01-07 03:15:00         0         0         0         0  \n",
       "...                       ...       ...       ...       ...  \n",
       "2022-06-27 09:15:00         0         0         0        -1  \n",
       "2022-06-27 10:15:00         0         0         0         0  \n",
       "2022-06-27 11:15:00         0         0         0         0  \n",
       "2022-06-27 12:15:00         0         0         0         0  \n",
       "2022-06-27 13:15:00         0         0         0         0  \n",
       "\n",
       "[8033 rows x 100 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"df_T\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba902a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a1dfc2f",
   "metadata": {},
   "source": [
    "## a) abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c7ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = \"4h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41218e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0=df2.copy()\n",
    "    \n",
    "n = int(len(df0)*0.7)\n",
    "iterations = 100\n",
    "\n",
    "df0 = df0.drop(columns=[\"threshold\", \"t1\"])\n",
    "df_train_pred = pd.DataFrame()\n",
    "df_train_pred.index = df0.index[:n]\n",
    "\n",
    "df_test_pred = pd.DataFrame()\n",
    "df_test_pred.index = df0.index[:n]\n",
    "# for tf in timeframes:\n",
    "tf= \"4h\"\n",
    "\n",
    "df_t = df0.copy().filter(regex=f\"{tf}|(label)\")\n",
    "df_t = df_t.drop(columns=[f\"{tf}_close\"])\n",
    "\n",
    "df_train_t = df_t.iloc[:n,:].copy()\n",
    "df_test_t = df_t.iloc[n:,:].copy()\n",
    "\n",
    "cat_features_t = list(df_train_t.filter(regex=\"tide\"))\n",
    "features_t = list(df_train_t.columns)[:-1]\n",
    "labels = [\"label\"]\n",
    "\n",
    "model = CatBoostClassifier(task_type=\"GPU\",iterations=iterations)\n",
    "model.fit(X=df_train_t[features_t], y=df_train_t[labels],cat_features = cat_features_t, verbose=False,plot=False)\n",
    "\n",
    "\n",
    "print(f\"{tf} ACCURACY ========\")\n",
    "train_y_pred = calc_accuracy(model, features_t, labels, df_train_t,df_test_t)\n",
    "df_train_pred[f\"{tf}_pred\"] = train_y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796e963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca06555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c03773cd",
   "metadata": {},
   "source": [
    "# 3) Cross validation\n",
    "Here we seek to improve the model by including cross validation instead of a single train test to validate the model across a rolling timeframe. We also employ catboost instead of xgboost due to the inherent abundance of categorical data in the dataset the paper works in. The model thus aims to produce allocation metrices using the probabilities predicted for each label (up down sideways next time step movement of the VBTX) We will then study the predicted values and see if its any use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5069cf23",
   "metadata": {},
   "source": [
    "The model walks forward on an anchored expanding window setting to predict the future VXBT state (-1,0,1). The parameters involved in this cross validation are \n",
    " - retrain_after_x_minutes: dictates the time elapsed before retraining the model\n",
    " - test_size : prediction window, for eg, currently set at 12 meaning, aside from predicting the next timestep t+5m, it will also predict for t+10m,t+15m ... t+60m using data available at t\n",
    " - output_size_desired is to start predicting from df.tail(80000) onwards\n",
    " - iterations is a boosting related parameter but we set at 100 as a start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f601c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils import indexable\n",
    "from sklearn.utils.validation import _num_samples\n",
    "\n",
    "class TimeSeriesSplitCustom(TimeSeriesSplit):\n",
    "    def __init__(self, n_splits=5, max_train_size=None,\n",
    "                 test_size=1,\n",
    "                 min_train_size=1,\n",
    "                 output_size_desired=1000,\n",
    "                 tail_start = True):\n",
    "        super().__init__(n_splits=n_splits, max_train_size=max_train_size)\n",
    "        self.test_size = test_size\n",
    "        self.min_train_size = min_train_size\n",
    "        self.tail_start = tail_start\n",
    "        self.output_size_desired = output_size_desired\n",
    "\n",
    "    def overlapping_split(self, X, y=None, groups=None):\n",
    "        min_train_size = self.min_train_size\n",
    "        test_size = self.test_size\n",
    "\n",
    "        n_splits = self.n_splits\n",
    "        n_samples = _num_samples(X)\n",
    "        # print(f\"to trigger TimeSeriesSplitCustom: {(n_samples - min_train_size) / test_size} >= {n_splits} must be false\")\n",
    "        if (n_samples - min_train_size) / test_size >= n_splits:\n",
    "            print('(n_samples -  min_train_size) / test_size >= n_splits')\n",
    "            print('default TimeSeriesSplit.split() used')\n",
    "            yield from super().split(X)\n",
    "\n",
    "        else:\n",
    "            print(\"Using TimeSeriesSplitCustom\")\n",
    "            shift = int(np.floor((n_samples - test_size - min_train_size) / (n_splits - 1)))\n",
    "\n",
    "            # start_test = n_samples - (n_splits * shift + test_size - shift)\n",
    "            if self.tail_start:\n",
    "                start_test = n_samples - output_size_desired\n",
    "                end_test = n_samples\n",
    "                print(f\"index start: {start_test}, index end: {end_test}\")\n",
    "            else:\n",
    "                start_test = min_train_size\n",
    "                end_test = min_train_size + self.output_size_desired\n",
    "                print(f\"index start: {start_test}, index end: {end_test}\")\n",
    "                \n",
    "            # test_starts = range(start_test, n_samples - test_size + 1, shift)\n",
    "            test_starts = range(start_test, end_test, shift)\n",
    "\n",
    "            if start_test < min_train_size:\n",
    "                raise ValueError(\n",
    "                    (\"The start of the testing : {0} is smaller\"\n",
    "                     \" than the minimum training samples: {1}.\").format(start_test,\n",
    "                                                                        min_train_size))\n",
    "\n",
    "            indices = np.arange(n_samples)\n",
    "\n",
    "            for test_start in test_starts:\n",
    "                if self.max_train_size and self.max_train_size < test_start:\n",
    "                    yield (indices[test_start - self.max_train_size:test_start],\n",
    "                           indices[test_start:test_start + test_size])\n",
    "                else:\n",
    "                    yield (indices[:test_start],\n",
    "                           indices[test_start:test_start + test_size])\n",
    "                    \n",
    "class TimeSeriesCrossValidator:\n",
    "    def __init__(self,\n",
    "                 df0, \n",
    "                 features=['vxbt_sig','tweet_vol_sig', 'sentiment_sig', 'gtrend_sig', 'index_sig'],\n",
    "                 cat_features = [\"tide\"],\n",
    "                 labels = [\"vxbt_sig_future\"], \n",
    "                 output_size_desired = 10, \n",
    "                 test_size = 2,\n",
    "                 iterations = 100,\n",
    "                 retrain_after_x_bars = 100,\n",
    "                 min_train_size=12*24*7,\n",
    "                 tail_start=False):\n",
    "        self.df = df0.copy()\n",
    "        self.colnames = list(df0.columns)\n",
    "        self.features = features\n",
    "        self.cat_features = cat_features\n",
    "        self.labels = labels\n",
    "        self.output_size_desired = output_size_desired\n",
    "        self.test_size = test_size\n",
    "        self.iterations = iterations\n",
    "        self.retrain_after_x_bars = retrain_after_x_bars\n",
    "        self.tail_start = tail_start\n",
    "        # (n_samples - min_train_size) / test_size < n_splits\n",
    "        # ----- vary n_splits such that we get output_size_desired\n",
    "        # len(df) -  min_train_size < test_size*n_splits\n",
    "        # output_size_desired/10 < n_splits\n",
    "        self.min_train_size = min_train_size #len(df)- self.output_size_desired - self.test_size\n",
    "        self.n_splits =  (len(df) - self.test_size - min_train_size)+1\n",
    "        \n",
    "        shift = int(np.floor((len(df) - self.test_size - min_train_size) / (self.n_splits - 1)))\n",
    "#         print(f\"shift: {shift}\")\n",
    "#         print(f\"sample size: {len(df)}\")\n",
    "#         print(f\"min_train_size: {min_train_size}\")\n",
    "#         print(f\"number of splits: {self.n_splits}\")\n",
    "        \n",
    "        self.cv = TimeSeriesSplitCustom(n_splits = self.n_splits,\n",
    "                                        test_size=self.test_size,\n",
    "                                        min_train_size=min_train_size,\n",
    "                                        tail_start=self.tail_start,\n",
    "                                        output_size_desired = self.output_size_desired)\n",
    "        \n",
    "    def build(self,df_train):\n",
    "        model = CatBoostClassifier(task_type=\"GPU\",iterations=self.iterations)\n",
    "        model.fit(X=df_train[self.features], y=df_train[self.labels],cat_features = self.cat_features, verbose=False,plot=False)\n",
    "        return model\n",
    "    \n",
    "    def run(self):\n",
    "        predictions = {}\n",
    "        probs_down = {}\n",
    "        probs_side = {}\n",
    "        probs_up = {}\n",
    "        retrain_alert = self.retrain_after_x_bars\n",
    "        for train_index, test_index in tqdm(self.cv.overlapping_split(self.df)):\n",
    "            df_train, df_test = self.df.iloc[train_index].copy(), self.df.iloc[test_index].copy()\n",
    "            #print(f\"TRAIN: {df_train.index[0]} -> {df_train.index[-1]}, TEST: {df_test.index[0]} -> {df_test.index[-1]}\")\n",
    "\n",
    "            # scale for each training set\n",
    "            # df_scaled[df_scaled.columns]=scaler.fit_transform(df_raw)\n",
    "            \n",
    "            # only retrain the model after 24hrs\n",
    "            if retrain_alert >= self.retrain_after_x_bars:\n",
    "                retrain_alert = 0\n",
    "                self.model = self.build(df_train)\n",
    "            \n",
    "            \n",
    "            predictions_t = self.model.predict(df_test[self.features])\n",
    "            probs_t = self.model.predict_proba(df_test[self.features])\n",
    "            \n",
    "            \n",
    "            i = df_test[self.features].index[0]\n",
    "            predictions[i] = {f\"pred_t{(t+1)*1}\":i for t,i in enumerate(predictions_t.T[0])}\n",
    "            probs_down[i] = {f\"prob_d_t{(t+1)*1}\":i for t,i in enumerate(probs_t.T[0])}\n",
    "            probs_side[i] = {f\"prob_s_t{(t+1)*1}\":i for t,i in enumerate(probs_t.T[1])}\n",
    "            probs_up[i] = {f\"prob_u_t{(t+1)*1}\":i for t,i in enumerate(probs_t.T[2])}\n",
    "            \n",
    "            retrain_alert += 1\n",
    "        \n",
    "\n",
    "        pred_df = pd.DataFrame.from_dict(predictions).T\n",
    "        prob_d_df = pd.DataFrame.from_dict(probs_down).T\n",
    "        prob_s_df = pd.DataFrame.from_dict(probs_side).T\n",
    "        prob_u_df = pd.DataFrame.from_dict(probs_up).T\n",
    "        \n",
    "        df = pd.merge(self.df, pred_df, left_index=True, right_index=True, how=\"left\")\n",
    "        df = pd.merge(df, prob_u_df, left_index=True, right_index=True, how=\"left\")\n",
    "        df = pd.merge(df, prob_s_df, left_index=True, right_index=True, how=\"left\")\n",
    "        df = pd.merge(df, prob_d_df, left_index=True, right_index=True, how=\"left\")\n",
    "        \n",
    "        # tidy up\n",
    "        \n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95ebc4",
   "metadata": {},
   "source": [
    "### ii) Run cross-validation for 1st half of dataset\n",
    "We perform another layer of validation to construct allocation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aca3b0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============\n",
    "# build model\n",
    "# ============\n",
    "n= 3100\n",
    "cat_features = list(df.filter(regex=\"tide\"))\n",
    "features = list(df_train.columns)[:-1]\n",
    "labels = [\"label\"]\n",
    "\n",
    "tail_start = False\n",
    "output_size_desired = 3000 # this will produce 80000 rows of predictions\n",
    "test_size = 2\n",
    "iterations = 100\n",
    "retrain_after_x_bars = 5*5*4 # retrain sample must not have data imbalance! if only labels 0,1 available then gg liao, predict will only output 2 \n",
    "min_train_size = 5*5*4\n",
    "tscv = TimeSeriesCrossValidator(df0 = df, \n",
    "                                features=features,\n",
    "                                cat_features = cat_features,\n",
    "                                labels = labels,\n",
    "                                output_size_desired=output_size_desired,\n",
    "                                test_size = test_size,\n",
    "                                iterations = iterations,\n",
    "                                min_train_size = min_train_size,\n",
    "                                retrain_after_x_bars = retrain_after_x_bars,\n",
    "                                tail_start=tail_start)\n",
    "res = tscv.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0cc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pip install pandas==1.4.3\n",
    "def pickle_data(data=None):\n",
    "    w = ['rb' if data is None else 'wb'][0]\n",
    "    with open(f'./database/catboost_data.pickle', w) as handle:\n",
    "        if data is None:\n",
    "            data = pickle.load(handle)\n",
    "            return data\n",
    "        else: \n",
    "            pickle.dump(data, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1eac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_data(data={\"df\":df, \"df2\":df2, \"res\":res})\n",
    "# df,df2,res = pickle_data().values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = res.copy()\n",
    "res1[\"4h_close\"] = df2[\"4h_close\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5dbcbc",
   "metadata": {},
   "source": [
    "### a) Analysis of data used\n",
    "Features constructed in the paper include \n",
    " - 5 minutely implied volatility time series dubbed the VXBT, taking cues from VIX\n",
    " - 5 minutely google search interests for \"bitcoin\"\n",
    " - 5 minutely number of tweets (excluding retweets) for \"bitcoin\"\n",
    " - 5 minutely tweet sentiment score on \"bitcoin\" related tweets   \n",
    " \n",
    "Their target/labels to fit their boosting model to the next time step of the VXBT index. We first plot out the VXBT and price and some volatility measures to ascertain its efficacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install plotly\n",
    "# pip install plotly-resampler \n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from plotly_resampler import FigureWidgetResampler\n",
    "\n",
    "class plotly_studies:\n",
    "    def __init__(self,\n",
    "                 row_heights: list=[1,2],\n",
    "                 cols_to_plot = [\"index\",\"label\"],\n",
    "                 height: int = 500,\n",
    "                 width: int = 500\n",
    "                ):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.number_subplots = len(cols_to_plot)\n",
    "        self.row_heights = row_heights\n",
    "        self.cols_to_plot = cols_to_plot\n",
    "        self.fig = FigureWidgetResampler(make_subplots(rows=self.number_subplots,\n",
    "                                                       cols=1, \n",
    "                                                       shared_xaxes = True, \n",
    "                                                       vertical_spacing = 0.05,\n",
    "                                                       row_heights = self.row_heights, \n",
    "                                                       specs =[[{\"type\":\"scatter\"}]]*self.number_subplots,\n",
    "                                                       subplot_titles = self.cols_to_plot))\n",
    "    def build(self,df):\n",
    "        row=1    \n",
    "        for col in self.cols_to_plot:\n",
    "            if (\"sig\" in col) or (len(df[col].unique()) < 5):\n",
    "                print(f\"{col} -> cat data detected\")\n",
    "                ax = go.Scattergl(x=df.index, y=df[col],name=col, mode='markers', marker = dict(size=4))\n",
    "                self.fig.append_trace(ax,row=row,col=1)\n",
    "                row+=1\n",
    "            else:\n",
    "                print(col)\n",
    "                # Note: plotly_resampler only supports scattergl so other go objected will not be resampled\n",
    "                ax = go.Scattergl(x=df.index, y=df[col],name=col)\n",
    "                self.fig.append_trace(ax,row=row,col=1)\n",
    "                row+=1\n",
    "\n",
    "        self.fig.update_layout(autosize=False,width=self.width,height=self.height)\n",
    "        return self.fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dd92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = plotly_studies(cols_to_plot = [\"label\",\"prob_u_t1\",\"prob_s_t1\",\"prob_d_t1\",\"pred_t1\",\"3h_close\"], row_heights = [1,0.5,0.5,0.5,1,3,], height=1000,width=1000)\n",
    "ps.build(res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03fbdc",
   "metadata": {},
   "source": [
    "### ii) Analyses\n",
    "We ascertain the efficacy of the model by looking at the accuracies of all the predictives. We can see that the t+5m and the t+10m predictions yield better accuracy than the other more forward predictions. So we can utilise there 2 prediction vectors to construct our asset allocation model. However, looking at the plot above, holding periods doesn't seem to be that long and the model could be dampened by slippage and fees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8218305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def get_accuracy(res):\n",
    "    df1 = res.dropna().copy()\n",
    "    for i in range(1,3):\n",
    "        print(f\"{'='*10}\")\n",
    "        labels=[-1,0,1]\n",
    "        cm = pd.DataFrame(metrics.confusion_matrix(df1[\"label\"], df1[f\"pred_t{i}\"],labels=labels))\n",
    "        cm.index=labels\n",
    "        cm.columns=labels\n",
    "        cm.index.name=f'actual {i}m'\n",
    "        cm.columns.name=f'predicted {i}m'\n",
    "        print(cm)\n",
    "        print(np.round(metrics.accuracy_score(df1[\"label\"], df1[f\"pred_t{i}\"]),3))\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = get_accuracy(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f6d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorised_backtest(df,fee=0.0001, holding_period=10):\n",
    "    df1=df.copy()\n",
    "    df1[\"returns\"] = np.log(df1['3h_close'] / df1['3h_close'].shift(1))\n",
    "\n",
    "    # long only\n",
    "    df1[\"position\"] =np.where(df1[\"prob_u_t1\"]>0.7,1,0)# + np.where(df1[\"prob_d_t1\"]>0.9,1,-1) \n",
    "\n",
    "    df1[\"strategy\"] = df1[\"position\"].shift(1) * df1[\"returns\"] - abs(df1[\"position\"].diff()).fillna(0)*fee\n",
    "    df1[[\"strategy\",\"returns\"]]=df1[[\"strategy\",\"returns\"]].cumsum().apply(np.exp)\n",
    "    df1[[\"strategy\",\"returns\"]].plot()\n",
    "    no_trades = abs(df1[\"position\"].diff().dropna()).astype(int).sum()\n",
    "    print(f\"Number of trades: {no_trades}\")\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "res11 = vectorised_backtest(res1.dropna(subset=[\"pred_t1\"]),0.001,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28185dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df11[\"position\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53300529",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(df11[\"position\"].diff().dropna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(df11[\"position\"].diff().dropna()).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd2cd3",
   "metadata": {},
   "source": [
    "We create a signal using the t+5m and t+10m predictions and run vectorised backtests and we can see that model some fantastic returns. However it has nearly 27000 trades over the course of 40000 5-minute bars. Which would kill the model with higher fees as shown below. However, let us assume we have 10bps of fees and check the if the model works on the final out-of-sample set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be114f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df12 = vectorised_backtest(df1,0.001,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c22bd7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ps = plotly_studies(cols_to_plot = [\"position\",\"strategy\",\"returns\",\"price\"], row_heights = [0.5,1,1,1], height=1000,width=1000)\n",
    "ps.build(df11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc10bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fb27b76",
   "metadata": {},
   "source": [
    "We run through the cross-validation to fit the model again using the latter half of the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============\n",
    "# build model\n",
    "# ============\n",
    "\n",
    "features=['vxbt_sig','tweet_vol_sig', 'sentiment_sig', 'gtrend_sig', 'price_sig']\n",
    "labels = [\"vxbt_sig_future\"]\n",
    "tail_start = True\n",
    "output_size_desired = 40000 # this will produce 80000 rows of predictions\n",
    "test_size = 12\n",
    "iterations = 100\n",
    "retrain_after_x_minutes = 5*12*24\n",
    "tscv = TimeSeriesCrossValidator(df0 = df, \n",
    "                                features=features,\n",
    "                                labels = labels,\n",
    "                                output_size_desired=output_size_desired,\n",
    "                                test_size = test_size,\n",
    "                                iterations = iterations,\n",
    "                                retrain_after_x_minutes = retrain_after_x_minutes,\n",
    "                                tail_start=tail_start)\n",
    "res2 = tscv.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c741c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = get_accuracy(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df21= vectorised_backtest(df2,0.0001,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df22 = vectorised_backtest(df2,0.001,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10464dbd",
   "metadata": {},
   "source": [
    "And yes we do see some good performance out of sample with 10bps of fees but the model totally flops with higher fees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1655a46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1705f15a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bd9d709",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "With more than 1 trade per 10minutes, and given sufficient market slippage and fees, the model will suffer from losses, if 10bps is not attainable. However, there could be ways to make the signal more stable and at the bottom you can see a starting draft of the triple barrier to construct the labels. With a longer enough horizon on the barrier, we can organically lengthen the holding periods of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f329893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class tripleBarrier:\n",
    "    def __init__(self,df: pd.DataFrame,\n",
    "                 col_name:str,\n",
    "                 vol_lookback: int = 100,\n",
    "                 vol_delta: pd.Timedelta = pd.Timedelta(hours=1),\n",
    "                 horizon_delta: pd.Timedelta = pd.Timedelta(minutes=15),\n",
    "                 barrier_multiplier: list = [1,1]\n",
    "                ):\n",
    "        self.df = df\n",
    "        self.col_name = col_name\n",
    "        self.vol_lookback = vol_lookback\n",
    "        self.vol_delta = vol_delta\n",
    "        self.horizon_delta = horizon_delta\n",
    "        self.barrier_multiplier = barrier_multiplier\n",
    "        \n",
    "    \"\"\"\n",
    "    main method: assignTripleBarrier()\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    def get_vol(self,data_col, vol_lookback,vol_delta):\n",
    "        prices = data_col.copy()\n",
    "        df0 = prices.index.searchsorted(prices.index - vol_delta)\n",
    "        df0 = df0[df0 > 0]  # 1.2 align timestamps of p[t-1] to timestamps of p[t]\n",
    "        df0 = pd.Series(prices.index[df0-1],   \n",
    "               index=prices.index[prices.shape[0]-df0.shape[0] : ])  # 1.3 get values by timestamps, then compute returns\n",
    "        df0 = prices.loc[df0.index] / prices.loc[df0.values].values - 1 \n",
    "        # 2. estimate rolling standard deviation\n",
    "        df0 = df0.ewm(span=vol_lookback).std()\n",
    "        return df0\n",
    "    \n",
    "    def get_horizons(self,prices, horizon_delta=pd.Timedelta(minutes=15)):\n",
    "        t1 = prices.index.searchsorted(prices.index + horizon_delta)\n",
    "        t1 = t1[t1 < prices.shape[0]]\n",
    "        t1 = prices.index[t1]\n",
    "        t1 = pd.Series(t1, index=prices.index[:t1.shape[0]])\n",
    "        t1.name = \"t1\"\n",
    "        return t1\n",
    "    def get_touches(self,prices, events, barrier_multiplier: list):\n",
    "        '''\n",
    "        events: pd dataframe with columns\n",
    "        t1: timestamp of the next horizon\n",
    "        threshold: unit height of top and bottom barriers\n",
    "        side: the side of each bet\n",
    "        barrier_multiplier: multipliers of the threshold to set the height of \n",
    "               top/bottom barriers\n",
    "        '''\n",
    "        out = events[['t1']].copy(deep=True)\n",
    "        if barrier_multiplier[0] > 0: \n",
    "            thresh_uppr = barrier_multiplier[0] * events['threshold']\n",
    "        else:\n",
    "            thresh_uppr = pd.Series(index=events.index) # no uppr thresh\n",
    "        if barrier_multiplier[1] > 0:\n",
    "            thresh_lwr = -barrier_multiplier[1] * events['threshold']\n",
    "        else:\n",
    "            thresh_lwr = pd.Series(index=events.index)  # no lwr thresh\n",
    "        for loc, t1 in tqdm(events['t1'].iteritems()):\n",
    "            df0=prices[loc:t1]                              # path prices\n",
    "            df0=(df0 / prices[loc] - 1) * events.side[loc]  # path returns\n",
    "            out.loc[loc, 'touch_top'] = df0[df0 < thresh_lwr[loc]].index.min()  # earliest touch_bot\n",
    "            out.loc[loc, 'touch_bot'] = df0[df0 > thresh_uppr[loc]].index.min() # earliest touch_top\n",
    "        return out\n",
    "    \n",
    "    def get_labels(self,touches):\n",
    "        out = touches.copy(deep=True)\n",
    "        # pandas df.min() ignores NaN values\n",
    "        first_touch = touches[['touch_bot', 'touch_top']].min(axis=1)\n",
    "        for loc, t in tqdm(first_touch.iteritems()):\n",
    "            if pd.isnull(t):\n",
    "                out.loc[loc, 'label'] = 0\n",
    "            elif t == touches.loc[loc, 'touch_bot']: \n",
    "                out.loc[loc, 'label'] = -1\n",
    "            else:\n",
    "                out.loc[loc, 'label'] = 1\n",
    "        return out\n",
    "    \n",
    "    def calcTripleBarrier(self):\n",
    "        data = self.df.copy()\n",
    "        data = data.assign(threshold=self.get_vol(data[self.col_name],\n",
    "                                                  vol_lookback = self.vol_lookback,\n",
    "                                                  vol_delta = self.vol_delta).dropna())\n",
    "        t1 = self.get_horizons(data, horizon_delta = self.horizon_delta)\n",
    "        data = pd.merge(data, t1, left_index=True, right_index=True, how=\"left\").dropna()\n",
    "        events_raw = data[['t1', 'threshold']] \n",
    "        events = events_raw.assign(side=pd.Series(-1., events_raw.index)) # long only\n",
    "        touches = self.get_touches(data[self.col_name], events, self.barrier_multiplier)\n",
    "        touches = self.get_labels(touches)\n",
    "        data = data.assign(label=touches.label)\n",
    "        \n",
    "        # for debugging\n",
    "        self.events_raw = events_raw\n",
    "        self.events = events\n",
    "        self.touches =touches\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff18c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tB = tripleBarrier(df_raw,\n",
    "                   col_name=\"vxbt\",\n",
    "                   vol_lookback = 10,\n",
    "                   vol_delta = pd.Timedelta(minutes=30),\n",
    "                   horizon_delta = pd.Timedelta(minutes=15),\n",
    "                   barrier_multiplier = [1,1])\n",
    "test = tB.calcTripleBarrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be8643",
   "metadata": {},
   "source": [
    "# Scratch Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5261fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb0360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc2f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.array(list(map(int, l2.split())))\n",
    "results_i = (np.abs(results - 0)).argmin()\n",
    "print(results[results_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58130fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7736f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c90e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas_ta as pta\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import sqlite3\n",
    "import time\n",
    "\n",
    "#%% Numba function definitions\n",
    "\n",
    "@nb.njit(parallel=True,cache=True)\n",
    "def numba_rolling_quantile(signal_array,q,winsize):\n",
    "    qtile = np.full(len(signal_array),np.nan)\n",
    "    # print(f\"size pctile: {len(pctile)}\")\n",
    "    \n",
    "    for i in nb.prange(winsize, len(signal_array)):\n",
    "        qtile[i] = np.quantile(signal_array[i-winsize+1:i+1], q[i])\n",
    "           \n",
    "    return qtile\n",
    "\n",
    "#%%\n",
    "\n",
    "pair = \"BTCUSDT\"  ## BTC/XRP/BNB/ETH\n",
    "timeframe = '1m'\n",
    "table_name = pair + '_' + timeframe + '_f'\n",
    "\n",
    "conn = sqlite3.connect('/Users/jeremykuek/Alphastone/quant_trading/database/binance_kline.db')\n",
    "query = f\"\"\" SELECT * from {table_name}\n",
    "            ORDER BY closeTime\n",
    "            \n",
    "        \"\"\" # LIMIT 10000\n",
    "        \n",
    "df_1m = pd.read_sql_query(query, conn)\n",
    "df_1m.drop_duplicates(subset=['closeTime'], inplace=True)\n",
    "\n",
    "\n",
    "# bar_interval = 1\n",
    "#%% 1m data validation\n",
    "\n",
    "# df_1m['close'].plot(title=f\"{pair} 1m close\")\n",
    "# plt.show()\n",
    "\n",
    "# df_1m['close'].isna().sum()\n",
    "\n",
    "# df_1m['close'].plot()\n",
    "# plt.show()\n",
    "\n",
    "#%%\n",
    "\n",
    "df_1m['date_time'] = pd.to_datetime(df_1m['closeTime'], unit='ms')\n",
    "df_1m.set_index(keys=['date_time'], inplace=True, drop=False)\n",
    "\n",
    "\n",
    "#%% 7,10,14,20,28,40,56,80\n",
    "\n",
    "scaling_factor = 1.0\n",
    "\n",
    "hour_List = np.array([7,10,14,20,28,40,56,80]) * scaling_factor\n",
    "min_List = [int(hour*60) for hour in hour_List]\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "df = pd.DataFrame(df_1m['close'], columns=['close'])\n",
    "\n",
    "df['logRet'] = np.log(1+df['close'].pct_change())\n",
    "df['logRet_norm'] = df['logRet'] / df['logRet'].rolling(10).std()\n",
    "\n",
    "# we need to fillna(0) because some pockets have zero mobvement in price => o/0 = nan\n",
    "df['logRet_norm'] = df['logRet_norm'].fillna(0)\n",
    "\n",
    "# df['logRet'].plot()\n",
    "# plt.show()\n",
    "\n",
    "# df['logRet_norm'].plot(title=\"logRet_norm\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "#%% memory of 500hrs =  =~ 20 days. introduce some memory of 500 hrs \n",
    "\n",
    "memory_minutes = 30000 # is 30k magic number ??\n",
    "\n",
    "df['logLevel_norm'] = df['logRet_norm'].rolling(memory_minutes).sum()\n",
    "\n",
    "#%%\n",
    "\n",
    "df['logLevel_norm'].plot(title='logLevels w/ 500hr memory')\n",
    "plt.show()\n",
    "\n",
    "# df['logRet_norm'].plot()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "#%% check for nan data\n",
    "\n",
    "df['logRet_norm'].isna().sum()\n",
    "df['logLevel_norm'].isna().sum()\n",
    "\n",
    "\n",
    "# df_copy = df['2021-03-01':'2021-04-01'] # to find blackout periods\n",
    "\n",
    "#%% now calculate all the slopes\n",
    "\n",
    "df_slopes = pd.DataFrame(index=df.index)\n",
    "slopeNames = []\n",
    "\n",
    "for minutes in min_List:\n",
    "    slope_name = 'slope_' + str(minutes)\n",
    "    slopeNames.append(slope_name)\n",
    "    df_slopes[slope_name] = (df['logLevel_norm'] - df['logLevel_norm'].shift(periods=minutes)) / minutes\n",
    "    \n",
    "\n",
    "df_slopes['slope_avg'] = df_slopes[slopeNames].mean(axis=1, skipna=False)\n",
    "\n",
    "#%%  Why is there a gap in the slope signal ???\n",
    "\n",
    "df_slopes['slope_avg'].plot()\n",
    "plt.show()\n",
    "\n",
    "#%% Now that we have an idea of average slopes, we want to find out the \n",
    "\n",
    "\n",
    "# select arbitrary quantiles for now 80-90 pctile is good\n",
    "upper_quantile = 0.90\n",
    "lower_quantile = round(1.0 - upper_quantile, 3)\n",
    "\n",
    "\n",
    "\n",
    "df_signal = pd.DataFrame(df_slopes['slope_avg'])\n",
    "\n",
    "df_signal['upper'] = df_signal['slope_avg'].rolling(memory_minutes).quantile(upper_quantile)\n",
    "df_signal['lower'] = df_signal['slope_avg'].rolling(memory_minutes).quantile(lower_quantile)\n",
    "\n",
    "\n",
    "#%% Visualize where the upper/lower slope quantiles are\n",
    "df_signal['upper'].plot()\n",
    "df_signal['lower'].plot()\n",
    "plt.show()\n",
    "\n",
    "#%%\n",
    "# df_signal['slope_avg'][\"2021-01-01\":].plot()\n",
    "\n",
    "#%%\n",
    "\n",
    "## ENTRY CRITERIA\n",
    "np_upperQuantile = df_signal['upper'].values\n",
    "np_lowerQuantile = df_signal['lower'].values\n",
    "\n",
    "\n",
    "trd_fees = 0.0007 ## 4bp per trade on Binance Perp futures\n",
    "np_positions = np.full(len(df), np.nan)\n",
    "forceCloseOut = True\n",
    "\n",
    "\n",
    "np_positions = np.full(len(df), np.nan)\n",
    "acct_values = np.full(len(df), np.nan)\n",
    "np_ret = np.full(len(df), np.nan)\n",
    "long_ret = np.full(len(df), np.nan)\n",
    "short_ret = np.full(len(df), np.nan)\n",
    "\n",
    "np_avgSlope = df_signal['slope_avg'].values\n",
    "np_upperQuantile = df_signal['upper'].values\n",
    "np_lowerQuantile = df_signal['lower'].values\n",
    "np_closePx = df['close'].values\n",
    "\n",
    "\n",
    "# initialize state flags\n",
    "isLong  = False\n",
    "isShort = False\n",
    "isOpen  = False\n",
    "\n",
    "holdDur = []\n",
    "\n",
    "capital = 1000.0 # each ccyPair start with $1000\n",
    "\n",
    "\n",
    "for idx_row in range(len(np_positions)):\n",
    "        \n",
    "    if not isOpen:\n",
    "        if np_avgSlope[idx_row] > np_upperQuantile[idx_row]: # symmZ \n",
    "            np_positions[idx_row] = 1   ## Open LONG position\n",
    "            isLong = True\n",
    "            isOpen = True\n",
    "            openIdx = idx_row\n",
    "            \n",
    "            entry_Px =  np_closePx[idx_row] \n",
    "        \n",
    "        elif np_avgSlope[idx_row] < np_lowerQuantile[idx_row]: # -symmZ: #\n",
    "            np_positions[idx_row] = -1  ## Open SHORT position\n",
    "            isShort = True\n",
    "            isOpen = True\n",
    "            openIdx = idx_row\n",
    "            \n",
    "            entry_Px = np_closePx[idx_row] \n",
    "            \n",
    "    else: #isOpen == True\n",
    "        if isLong: \n",
    "            if np_avgSlope[idx_row]< 0 or (forceCloseOut and idx_row == len(np_avgSlope)-1): # -symmZ \n",
    "                np_positions[idx_row] = np.nan\n",
    "                isLong = False\n",
    "                isOpen = False\n",
    "                holdDur.append(idx_row-openIdx)\n",
    "                \n",
    "                exit_Px = np_closePx[idx_row]\n",
    "                discrete_Long_ret = (exit_Px / entry_Px) - (2 * trd_fees) ## 2x trd_fees for entry & exit\n",
    "                capital = capital * discrete_Long_ret                         ## update the new capital value\n",
    "                acct_values[idx_row] = capital\n",
    "                np_ret[idx_row] = discrete_Long_ret - 1\n",
    "                long_ret[idx_row] = discrete_Long_ret - 1\n",
    "            \n",
    "            else:\n",
    "                np_positions[idx_row] = np_positions[idx_row-1]\n",
    "                acct_values[idx_row] = capital\n",
    "            \n",
    "        elif isShort:\n",
    "            if np_avgSlope[idx_row] > 0 or (forceCloseOut and idx_row == len(np_avgSlope)-1): # symmZ\n",
    "                np_positions[idx_row] = np.nan\n",
    "                isShort = False\n",
    "                isOpen = False\n",
    "                holdDur.append(idx_row-openIdx)\n",
    "                \n",
    "                exit_Px = np_closePx[idx_row]\n",
    "                discrete_Short_ret = (entry_Px / exit_Px) - (2 * trd_fees) ## 2x trd_fees for entry & exit\n",
    "                capital = capital * discrete_Short_ret                         ## update the new capital value\n",
    "                acct_values[idx_row] = capital\n",
    "                np_ret[idx_row] = discrete_Short_ret - 1\n",
    "                short_ret[idx_row] = discrete_Short_ret - 1\n",
    "            \n",
    "            else:\n",
    "                np_positions[idx_row] = np_positions[idx_row-1]\n",
    "                acct_values[idx_row] = capital\n",
    "            \n",
    "\n",
    "n_trades = len(holdDur)\n",
    "print(f\"total # of trades: {n_trades}\")\n",
    "    \n",
    "#%%\n",
    "\n",
    "df['positions'] = np_positions # pd.DataFrame(np_positions,index=df.index)\n",
    "df['minutePnl'] = df['positions'].shift() * df['close'].pct_change() \n",
    "\n",
    "\n",
    "#%% \n",
    "\n",
    "df['cumsum_minutePnl'] = df['minutePnl'].cumsum()\n",
    "\n",
    "# takes into account PnL from minute-to-minute price variations\n",
    "df['cumsum_minutePnl'].plot(title=f'{pair} minutePnl cumsum')\n",
    "plt.show()\n",
    "\n",
    "#%% Discrete PnL from each trade. cannot cumsum without replacing nan with 0...hence we create 'discretePnl_plot'\n",
    "\n",
    "df['discretePnl'] = np_ret\n",
    "df['longPnl'] = long_ret\n",
    "df['shortPnl'] = short_ret\n",
    "\n",
    "df['discretePnl_plot'] = df['discretePnl'].fillna(0)  # needed to fillna(0) for cumsum in next step\n",
    "df['longPnl_plot'] = df['longPnl'].fillna(0) \n",
    "df['shortPnl_plot'] = df['shortPnl'].fillna(0) \n",
    "\n",
    "\n",
    "df['cumsum_discretePnl_plot'] = np.cumsum(df['discretePnl_plot'])\n",
    "df['cumsum_longPnl_plot'] = np.cumsum(df['longPnl_plot'])\n",
    "df['cumsum_shortPnl_plot'] = np.cumsum(df['shortPnl_plot'])\n",
    "\n",
    "\n",
    "# df['cumsum_discretePnl_plot'].plot(title=f'{pair} discretePnl cumsum')\n",
    "df['cumsum_discretePnl_plot'].plot(label=f'{pair} discretePnl')\n",
    "df['cumsum_longPnl_plot'].plot(label=f'{pair} longPnl')\n",
    "df['cumsum_shortPnl_plot'].plot(label=f'{pair} shortPnl')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# df['cumsum_discretePnl_plot']['2021-01-01':].plot(title=f'{pair} discretePnl cumsum')\n",
    "# plt.show()\n",
    "\n",
    "#%% histogram of returns\n",
    "\n",
    "df['discretePnl'].hist(bins=100)\n",
    "plt.title(f'{pair} discretePnl histogram')\n",
    "plt.show()\n",
    "\n",
    "#%% annl Sharpe Ratio. 60min/day * 24 hr/day * 365 days/year\n",
    "\n",
    "sharpe = np.sqrt(60*24*365) * df['minutePnl'].mean() / df['minutePnl'].std()\n",
    "print(f\"minute granular sharpe:{sharpe}\")\n",
    "\n",
    "#%% discrete sharpe\n",
    "\n",
    "\"\"\" we make N discrete trades over a span of Y years\n",
    "    we have to assume that \n",
    "\n",
    "\"\"\"\n",
    "n_days = (df.index[-1] - df.index[0]).days  # timedelta -> days\n",
    "\n",
    "\n",
    "#% Now we need to normalize to N number of trading events per year\n",
    "\n",
    "N = n_trades * (365/n_days)\n",
    "discrete_sharpe = np.sqrt(N) * np.nanmean(np_ret) / np.nanstd(np_ret)\n",
    "print(f\"discrete_sharpe:{discrete_sharpe}\")\n",
    "\n",
    "#%% win rate\n",
    "\n",
    "wins = (df['discretePnl']>0).sum()\n",
    "losses = (df['discretePnl']<0).sum()\n",
    "\n",
    "win_rate = wins / (wins + losses)\n",
    "print(f\"win_rate: {win_rate}\")\n",
    "\n",
    "#%% gains to pain\n",
    "\n",
    "gains = df['discretePnl'][df['discretePnl']>0].sum()\n",
    "pains = abs(df['discretePnl'][df['discretePnl']<0].sum())\n",
    "\n",
    "gain_to_pain = gains / pains\n",
    "\n",
    "print(f\"gains/pains:{gain_to_pain}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
